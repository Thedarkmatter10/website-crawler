# website-crawler

A fast and flexible **command-line tool** written in **Go** that crawls a website, checks its status, builds a site map, and exports it as **JSON** or **XML**.

---

## ğŸš€ Features

- âœ… Crawl entire website up to a specified depth
- âœ… Respect `robots.txt` rules
- âœ… Concurrency support for faster crawling(ABSENT AT the moment it will be added in future.)
- âœ… Rate-limited by default to avoid server overload
- âœ… Export site map in JSON or XML format
- âœ… Human-readable tree output in terminal
- âœ… Built-in profiler support (`pprof`)
- âœ… Well-tested with full integration test suite

---
## ğŸ› ï¸ Installation

> âš ï¸ Make sure you have Go installed on your system. You can download it from https://golang.org/dl/

```bash
git clone https://github.com/yourusername/website-crawler.git
cd website-crawler
go build -o crawler
```

---

## ğŸ”§ Usage

```bash
./crawler [URL] [flags]
```

If no URL is provided, it defaults to `https://example.com`.

---

## ğŸ“Œ Flags

| Flag             | Description                                 | Default        |
|------------------|---------------------------------------------|----------------|
| `--output`, `-o` | Export site map to file                     | (none)         |
| `--format`, `-f` | Output format: `json` or `xml`              | `json`         |
| `--depth`, `-d`  | Max crawl depth                             | `10`           |
| `--concurrency`, `-c` | Number of concurrent crawlers         | `10`           |
| `--version`      | Show CLI version                            |                |

---

## ğŸ§ª Examples

### âœ… Crawl and display site map
```bash
go run . https://example.com
```

### ğŸ“ Export site map to JSON
```bash
go run . https://example.com --output sitemap.json --format json
```

### ğŸ§¾ Export site map to XML
```bash
go run . https://example.com --output sitemap.xml --format xml
```

### ğŸ”„ Crawl with custom depth and concurrency
```bash
go run . https://example.com --depth 3 --concurrency 5
```

---

## âš™ï¸ Profiler

The tool includes optional `pprof` profiling on port `:6060`.
This runs **automatically** (except during testing):

```bash
go tool pprof http://localhost:6060/debug/pprof/profile
```

---

## ğŸ§ª Testing

Run the full test suite:
```bash
go test ./...
```

The tests spin up a live local server and validate:
- Real crawling
- Exported file content
- Flag handling
- Robots.txt blocking
- Edge cases

---

## ğŸ“ Example Output

```
[âœ“] Site: https://example.com

Status: 200 OK
Response Time: 123ms

Site Map:

- https://example.com
  - https://example.com/about
  - https://example.com/contact
```

---

---


